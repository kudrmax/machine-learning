{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Простая эвристика__: давайте выдавать такой же ответ, какой является правильным для объекта обучающей выборки, наиболее похожего на данный (это будет наша _модель ответа_). \n",
    "Такой классификатор называется __Метод ближайшего соседа__.\n",
    "\n",
    "В его основе лежит _предположение_ о том, что объекты одного класса лежат близко друг к другу в пространстве признаков (_гипотеза компактности_).\n",
    "\n",
    "Стоит обратить внимание, что это чуть ли не единственный метод машинного обучения, в котором процедура _обучения_ состоит в _запоминании_ выборки (не нужно ничего оптимизировать)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, легко понять, что для двумерного случая классификация всей плоскости задается разбиением ее на многоугольники, ограниченные серединными перпендикулярами к отрезкам, попарно соединяющим объекты обучающей выборки.\n",
    "Это показано на левой картинке (здесь выборка состоит из 8 объектов, целевой признак --- цвет точки: синий или зеленый, каждый объект представлен двумя признаками-координатами на вещественной плоскости)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://shapeofdata.files.wordpress.com/2013/03/knn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь кроется и вся опасность МБС --- его склонность к переобучению: для любого шумового объекта (например, правой нижней синей точки) будет создаваться отдельная область, то есть алгоритм крайне чувствителен к шуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте немного усложним классификатор: вместо одного соседа будем рассматривать k наиболее похожих на данный объект, и выбирать тот класс, который наиболее часто встречается среди k соседей (новая _модель_). Получится классификатор, более устойчивый к переобучению. Этот метод называется __метод k ближайших соседей, или KNN__. Например, на правой картинке показана разделяющая поверхность при k=3, и там уже синий объект признается шумовым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://datasciencetoday.net/images/demo/k_NN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обсуждение kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ряда задач (например, для распознавания цифр или классификации текстов, представленных в виде <<мешка слов>>) kNN дает хороший результат. Ключевой вопрос состоит в __выборе метрики для сравнения объектов__. Для вещественных объектов $x = (x_1, \\dots, x_d)$ и $y=(y_1, \\dots, y_d)$ используют:\n",
    "1. расстояние Минковского с показателем $p$: $\\rho(x, y) = \\biggl(\\sum_{i=1}^d (x_i - y_i)^p\\biggr)^{\\frac 1 p}$; при $p=2$ это привычное _евклидово расстояние_.\n",
    "1. косинусную метрику:  $\\rho(x, y) = \\frac {<x, y>} {\\sqrt{<x, x><y, y>}}$, $<. ,.>$ --- скалярное произведение: $<x, y> = \\sum_{i=1}^d x_i y_i$. Это расстояние равно косинусу угла между векторами $x$ и $y$ в евклидовом пространстве; хорошо работает для текстов.\n",
    "\n",
    "Для бинарных векторов (каждый компонент есть 0 или 1):\n",
    "1. расстояние Хэмминга --- число несовпадающих разрядов. \n",
    "1. расстояние Жаккарда --- число единиц в поэлементной конъюнкции, деленное на число элемент в поэлементной дизъюнкции.\n",
    "\n",
    "Последнее есть интерпретация расстояния Жаккарда для множеств A и B:\n",
    "1. $\\rho(A, B) = \\frac {|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "(Подробнее о метриках здесь: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics)\n",
    "\n",
    "Если в задаче известны признаки разной природы, можно составлять комбинированную метрику. \n",
    "\n",
    "Более того, можно вводить веса признаков $w = (w_1, \\dots, w_d)$ и модифицировать метрики, например:\n",
    "$\\rho(x, y) = \\sqrt{\\sum_{i=1}^d w_i(x_i - y_i)^2}$.\n",
    "\n",
    "Последний прием важен на _ненормированных_ выборках (если в сумме одно слагаемое имеет порядок 1000, а другое 0.1, то второе не будет иметь влияния на величину расстояния). Тогда в качестве весов можно использовать обратные к средним или к максимальным значениям признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса можно применять и к объектам, тогда объекты, находящиеся на расстоянии, более близком к новой точке, будут иметь большее влияние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN в sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерфейс описан в [документации](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем классификатор\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загружаем данные --- изображения цифр\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.images\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797L, 8L, 8L)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1) # вытягиваем квадратное изображение в вектор, чтобы получить матрицу объекты-признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797L, 64L), (1797L,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape # проверяем, что все хорошо перемешалось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 8, 6, 5, 2, 5, 6, 5, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10] # теперь в случайном порядке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:700, :], y[:700]\n",
    "X_val, y_val = X[700:1300, :], y[700:1300] #validation\n",
    "X_test, y_test = X[1300:, :], y[1300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обучаем классификатор и делаем предсказания\n",
    "clf.fit(X_train, y_train)\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.955734406439\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем простейшую метрику качества алгоритма --- долю правильных ответов\n",
    "print(\"Accuracy is\", np.mean(y_test==y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что у нас 10 классов, и вероятность случайно вытащить два одинаковых маленькая, точность 0.956 --- очень хороший результат!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 ; accuracy = 0.958333333333\n",
      "k = 2 ; accuracy = 0.956666666667\n",
      "k = 3 ; accuracy = 0.956666666667\n",
      "k = 4 ; accuracy = 0.956666666667\n",
      "k = 5 ; accuracy = 0.958333333333\n",
      "k = 6 ; accuracy = 0.958333333333\n",
      "k = 7 ; accuracy = 0.963333333333\n",
      "k = 8 ; accuracy = 0.965\n",
      "k = 9 ; accuracy = 0.965\n",
      "k = 10 ; accuracy = 0.966666666667\n",
      "k = 11 ; accuracy = 0.958333333333\n",
      "k = 12 ; accuracy = 0.961666666667\n",
      "k = 13 ; accuracy = 0.96\n",
      "k = 14 ; accuracy = 0.956666666667\n",
      "k = 15 ; accuracy = 0.951666666667\n",
      "k = 16 ; accuracy = 0.946666666667\n",
      "k = 17 ; accuracy = 0.94\n",
      "k = 18 ; accuracy = 0.94\n",
      "k = 19 ; accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "# Подбор k на валидационной выборке:\n",
    "for k in range(1, 20):\n",
    "    y_predicted = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train).predict(X_val)\n",
    "    print(\"k =\", k, \"; accuracy =\", np.mean(y_predicted==y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат при k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Плюсы и минусы метода ближайших соседей**\n",
    "\n",
    "Плюсы:\n",
    "\n",
    "\n",
    "1. Простая реализация;\n",
    "\n",
    "2. Неплохо изучен теоретически;\n",
    "\n",
    "3. Как правило, метод хорош для первого решения задачи, причем не только классификации или регрессии, но и, например, рекомендации;\n",
    "\n",
    "4. Можно адаптировать под нужную задачу выбором метрики или ядра (в двух словах: ядро может задавать операцию сходства для сложных объектов типа графов, а сам подход kNN остается тем же);\n",
    "\n",
    "5. Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так. Хотя этот аргумент можно атаковать: если число соседей большое, то интерпретация ухудшается (условно: \"мы не дали ему кредит, потому что он похож на 350 клиентов, из которых 70 – плохие, что на 12% больше, чем в среднем по выборке\").\n",
    "\n",
    "Минусы:\n",
    "\n",
    "\n",
    "1. Метод считается быстрым в сравнении, например, с композициями алгоритмов, но в реальных задачах, как правило, число соседей, используемых для классификации, будет большим (100-150), и в таком случае алгоритм будет работать не так быстро, как дерево решений;\n",
    "\n",
    "2. Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии;\n",
    "\n",
    "3. Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных это отнимает много времени;\n",
    "\n",
    "4. Нет теоретических оснований выбора определенного числа соседей — только перебор (впрочем, чаще всего это верно для всех гиперпараметров всех моделей). В случае малого числа соседей метод чувствителен к выбросам, то есть склонен переобучаться;\n",
    "\n",
    "5. Как правило, плохо работает, когда признаков много, из-за \"проклятия размерности\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
